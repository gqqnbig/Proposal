\chapter{Literature Review}



\section{State-of-the-art review}

To address the requirement elicitation issue, a tool that generates prototypes directly from requirements automatically is highly desired.
Since the unified modeling language (UML) is a de facto standard for requirements modeling and system design,
state-of-the-art research have been focusing on execution of UML models, i.e., turn UML into executable code~\cite{ciccozzi2019execution}.
However, the current UML modeling tools, such as Rational Rose, SmartDraw, MagicDraw, and Papyrus UML, can only generate skeleton code, where classes only contain attributes and signatures of operations, not their implementations~\cite{regep2000using}.

During the implementation phase, there are tools helping analyze source code~\cite{morgachev2019detection,huo2016learning,gu2016deep},
generate commit message from diff~\cite{linares2015changescribe,buse2010automatically,huang2020learning},
generate release notes from commits since the last release~\cite{moreno2016arena}, and so on.
%RefDiff~\cite{silva2020refdiff} and RMiner~\cite{tsantalis2018accurate} read the complete content of the changed files before and after a commit and construct a diff of an internal format, from which they detect refactoring types.


\section{Critical summary and analysis of key references}

\etal{Yang}~\cite{yang2019automated} proposed a set of transformation rules that decomposes a contract into executable parts and non-executable parts, and automatically generate an executable prototype in Java.
The resulting tool is called RM2PT implemented as Eclipse plugin. The input of RM2PT is a requirement document written in Yang's domain specific language, and the output of RM2PT is a JavaFX desktop application.

I conceive I can use a similar idea to RM2PT that generates a blockchain application out of a requirement document or ports a general Java application to a blockchain application.


When we are coding, we often need to do refactoring.
RefDiff~\cite{silva2020refdiff} and RMiner~\cite{tsantalis2018accurate} can detect refactoring types and help us write commit message. Nonetheless, they have to read complete content of the changed files before and after a commit in order to run a detection.

Since each commit can be concisely represented by a diff, I believe using diff as input for analysis is promising because the size of diff is much smaller than the whole solution and the analysis tool can run faster.




A method can be characterized by its input, techniques, and output.
The input of our work is diff files, the techniques are neural networks, and the output is classification.
Thus, in this section we first describe the format of diff files, and then introduce the related work in the aspects of input, techniques, and output.


\begin{figure}[t]
\begin{lstlisting}[language=diff, breaklines=true, numbers=left, xleftmargin=2em]
diff --git a/src/.../TldPatterns.java b/src/.../TldPatterns.java
old mode 100644
new mode 100755
diff --git a/src/.../Chars.java b/src/.../Chars.java
index a0cf5bd..e26dca8 100644
--- a/src/.../Chars.java
+++ b/src/.../Chars.java
@@ -43,7 +43,7 @@ import java.util.RandomAccess;
* @author Kevin Bourrillion
* @since 1
*/
- public class Chars {
+ public final class Chars {
	private Chars() {}
}
\ No newline at end of file
\end{lstlisting}
\caption{Example diff file in dataset. Line 8 to 15 are valid input to our model.}
\label{fig:example-diff}
\end{figure}

Both {\gnudiff} and {\gitdiff} are able to show the difference character by character between two text files or commits.
We concatenate the output so that changes to multiple files are recorded in a single diff file.
In terms of the diff format, a line starting with double at-symbol (\code{@@}) signals the starting of a hunk where the files differ.
Diff files generated by {\gitdiff} have optional hunk headers appended at the end of the \code{@@} lines.
Fig.~\ref{fig:example-diff} shows an example diff file.

Code summarization is a sub-field of natural language processing (NLP),
here the input is the program source code and the output is the summary.
A large body of work has been carried out in this field with assorted techniques.
Despite the fact that code summarization does not match our work in all the three aspects,
some observations inspired us.

Some techniques are shared from NLP to code summarization after \etal{Allamanis}~\cite{allamanis2018survey} proposed the naturalness hypothesis which expounded the similarities between programming languages and natural languages.
Researchers~\cite{xu2019commit,liu2019generating} working in commit message generation gradually found neural machine translation (NMT),
which was originally used for translating a natural language to another,
excelled at translating diff to commit messages.
Such an NMT approach has two major components, the encoder and the decoder.
The encoder reads the diff and encodes it to some internal matrix representation,
then the decoder transforms the internal representation to human readable text, i.e., the commit messages.
Commit classification can use a similar technique as in NMT,
but replacing the decoder to other neural layers.

When the input is source code rather than diff,
the field becomes program analysis, where more and more work has employed neural network techniques.
\etal{Alexandru}~\cite{alexandru2017replicating} used NMT to annotate source code tokens with typing,
and they also implemented an {\sc antlr}-based parser.
\textsc{JSNice}~\cite{raychev2015predicting} is a neural network model that predicts names of JavaScript identifiers and type annotations of variables.
\etal{Mou}~\cite{mou2016convolutional} used a convolutional neural network to classify programs by functionalities, such as string manipulations and matrix operations.
Other uses of neural networks in program analysis include
detection of variable misuse~\cite{morgachev2019detection},
bug localization~\cite{huo2016learning},
API suggestion~\cite{gu2016deep}, and
code completion~\cite{raychev2014code}.

Taking diff as input, program analysis is specialized to diff analysis.
\etal{Moreno}~\cite{moreno2016arena} generated release notes by reading changes to program source code and documentations, as well as taking into consideration issues from software repositories.
ChangeScribe~\cite{linares2015changescribe},
ChangeDoc~\cite{huang2020learning}, and
DeltaDoc~\cite{buse2010automatically} read diff and followed a set of rules to generate commit messages.
None of them used machine learning, and howbeit they all processed diff,
their goal was not to classify the diff.
They do not accept the {\gitdiff} format which shows barely changed lines of changed files.

Later, machine learning techniques emerged in diff analysis.
\etal{Loyola}~\cite{loyola2017neural} developed a neural network model to generate text description from diff files.
Their work relies on a lexer that divides source code into tokens,
thus it is not end-to-end machine learning.
\etal{Macho}~\cite{macho2016predicting} employed a random forest classifier and categorized commits into forward engineering, re-engineering, corrective engineering and management.
By contrast, our work labels commits into their refactoring types.








There are existing works in the field of program analysis,
but research is struggling to answer questions like ``Why does this method crash?'', ``Is this code thread-safe?''~\cite{alexandru2017replicating}.
To enrich the experience towards solving the broader problem of code classification,
we carry out this experiment to tackle one specific type of code modification recognition
--- renaming of identifiers.
In this work, we propose a probabilistic machine-learning classification model,
per taxonomy of~\cite{allamanis2018survey},
to automatically identify the commits to a version-control system that only consist of identifier renaming.
This technique is novel in that the neural network is able to read merely the diff and give a confidence value of whether it belongs to the category.
On the contrary, the traditional way of detecting such changes is done by analyzing the full source code before and after the commit,
which is less efficient and often complicated with partial source code.
For typical software projects nowadays each with a lot of imported libraries,
the ability to carry out analysis based on local partial source code is certainly an advantage.

Our model can be used as a component in the Continuous Integration (CI) with a Pull-Request (PR) workflow, for instance, on GitHub Actions.
When a developer submits a pull request, CI runs to build and test the code and can add labels to the PR.
Our model plays a role in CI that we take the unified diff of the PR as input,
and classify whether the diff is identifier renaming, ``yes'' or ``no''.
Then, CI can label the PR or assign it to the corresponding code reviewers.
For our model to work, if one commit changes multiple files,
the changes to these files should be concatenated to a single diff file,
as the~{\gitdiff} program does.

Our work focuses on the classification of diff in a typical programming language
--- Java diff files,
because Java is a popular language and the syntax is relatively simple.
We specifically target Java 7, since Java 8 and beyond have introduced many new syntactical improvements,
which could cause complication in the parsing and needlessly shift our focus away.

There is no off-the-shelf dataset of commits and labels of whether the commit is identifier renaming.
\etal{Jiang}~\cite{jiang2017} collected an unlabeled dataset of \num{1006} repositories and over 2 million commits from GitHub.
Although \etal{Jiang} claimed their dataset contained only Java source code,
we found it actually containing many non-Java contents.
We filtered out non-Java code and 62K commits remain.
To further expand the dataset, we employed the crawling GitHub tool provided by~\cite{alexandru2017replicating} to collect additional commits from top Java repos on GitHub in January 2021.
The final dataset obtained has \num{73080} examples.

We contribute in this work a probabilistic neural network classification model with diff files as input, telling whether the input is renaming or not.
The model was specifically trained against the Java programming language, and
we believe minor tweaks can allow it to work on other programming languages.
Besides the neural network model, we also present a dataset that labels 73K Java diff files with whether they are renaming commits,
together with a syntax analyzer for Java diff files based on {\sc antlr}.






\begin{equation}\label{eq:sample1}
	a^2 + b^2 = c^2
\end{equation}

\begin{equation}\label{eq:sample2}
	\begin{dcases}
		a + b = 2\\
		a - b = 0
	\end{dcases}
	\implies
	\begin{dcases}
		a = 1\\
		b = 1
	\end{dcases}
\end{equation}

\autoref{eq:sample1} and \ref{eq:sample2} as sample equation.


\begin{table} [h]
	\centering
	\caption{Sample of table.}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\
		\hline
		1 & 2 & 3 & 4 \\
		\hline
		5 & 6 & 7 & 8 \\
		\hline
	\end{tabular}
	\label{tab:sample}
\end{table}

\autoref{tab:sample} as sample table.