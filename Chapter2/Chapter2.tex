\chapter{Literature Review}



\section{State-of-the-art review}

To address the requirement elicitation issue, a tool that generates prototypes directly from requirements automatically is highly desired.
Since the unified modeling language (UML) is a de facto standard for requirements modeling and system design,
state-of-the-art research have been focusing on execution of UML models, i.e., turn UML into executable code~\cite{ciccozzi2019execution}.
In the past, UML modeling tools, such as Rational Rose, SmartDraw, MagicDraw, and Papyrus UML, can only generate skeleton code, where classes only contain attributes and signatures of operations, with method body empty~\cite{regep2000using}.

In recent years, the code generation task leverages neural networks.
There are extensive research on Natural Language Processing in combination with neural networks. \etal{Allamanis}~\cite{allamanis2018survey} proposed the naturalness hypothesis which expounded the similarities between programming languages and natural languages. Since then, more and more work in software engineering start to use neural networks, in particular neural machine translation and transformers.
Grammformers~\cite{guo2021learning} made an attempt in using transformers. It takes the context text and generates next code statements. The generated statements may contain holes where Grammformers is uncertain about and leaves for developers to fill in.

When we generate source code, we have to make sure the generated code is correct, which falls under the field of software verification.
Verifying smart contract has been a research focus over the past several years thanks to the heat and the market cap of BitCoin and Ethereum.
\cite{tolmach2021survey}~summarized the two steps on verifying the correctness of smart contract and presented related works.
First, we need to formalize the requirements, i.e., we have to build formal specifications for smart contracts.
Then we read the implementation of the smart contract, abstract it to a formal mathematical model, and check it against the specification.


During the implementation phase, there are tools helping analyze source code~\cite{morgachev2019detection,huo2016learning,gu2016deep},
generate commit message from diff~\cite{linares2015changescribe,buse2010automatically,huang2020learning},
generate release notes from commits since the last release~\cite{moreno2016arena}, and so on.
RefDiff~\cite{silva2020refdiff} and RMiner~\cite{tsantalis2018accurate} read the complete content of the changed files before and after a commit and construct a diff of an internal format, from which they detect refactoring types.


\section{Datasets}
In requirement engineering, several use cases (datasets) were proposed.
The Common Component Modeling Example (\cocome)~\cite{herold2008cocome} describes a trading system in a supermarket setting.
The system has cash desks that scan products and allow customers to pay by credit card or cash. It also performs administrative tasks like order products, open or close stores, etc.
{\cocome} is basically a local, single-user system with no parallelism handling.
SLEX-Web~\cite{jantan2012extension} is a web application for school. It not only offers information about department, programs, courses, and researchers, but also features interactions between students and teachers via an e-learning system as well as quizzes.


In order to analyze source code with neural networks, datasets are required to train the networks. The source code repositories on GitHub are often utilized.
\etal{Jiang}~\cite{jiang2017} collected a dataset of \num{1006} repositories and over 2 million commits from GitHub. This dataset is unlabeled and the code is mostly in Java language with other languages mixed in.
The dataset created by~\cite{tsantalis2018accurate} focuses on refactoring types. It comprises \num{3188} refactorings found in 538 commits from 185 open-source projects. The dataset is now hosted on \url{https://github.com/aserg-ufmg/RefDiff}. In this dataset, one commit may contain multiple refactoring types and functional changes are permitted. For example, as long as one method is renamed among a great deal of functional changes, the commit is labeled as method renaming.

Both {\gnudiff} and {\gitdiff} are able to show the difference character by character between two text files or commits.
Changes to multiple files can be recorded in a single diff file.
In terms of the diff format, a line starting with double at-symbol (\code{@@}) signals the starting of a hunk where the files differ.
Diff files generated by {\gitdiff} have optional hunk headers appended at the end of the \code{@@} lines.



\section{Critical summary and analysis of key references}

\etal{Yang}~\cite{yang2019automated} proposed a set of transformation rules that decomposes a requirement document into executable parts and non-executable parts, and automatically generate an executable prototype in Java.
The resulting tool is called RM2PT implemented as Eclipse plugin. The input of RM2PT is a requirement document written in Yang's domain specific language (DSL), and the output is a JavaFX desktop application. RM2PT is able to complete 93.65\% system operations on average while the others are not generatable due to the involvement of third-party API or sorting.

I conceive a similar idea to RM2PT of generating a blockchain application out of a requirement document or porting a general Java application to a blockchain application.

After we got a working prototype, we need to add missing functions, optimize it, and refactor it into our desired application.
RefDiff~\cite{silva2020refdiff} and RMiner~\cite{tsantalis2018accurate} can detect refactoring types and help us write commit message. Nonetheless, they have to read complete content of the changed files before and after a commit in order to run a detection.

Since each commit can be concisely represented by a diff, I believe using diff as input for analysis is promising because the size of diff is much smaller than the whole solution and the analysis tool can run much faster.